{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c086f252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8e1027a5b6e454d9eea2caf496761d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import FuyuProcessor, FuyuForCausalLM, BitsAndBytesConfig\n",
    "from PIL import Image\n",
    "\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "# load model and processor\n",
    "CACHE_DIR = \"./cache\"\n",
    "model_id = \"adept/fuyu-8b\"\n",
    "\n",
    "# The current model is not compatible with 4-bit/8-bit quantization.\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True, \n",
    "#     bnb_4bit_use_double_quant=True, \n",
    "#     bnb_4bit_quant_type=\"nf4\", \n",
    "#     bnb_4bit_compute_dtype=torch.float16,\n",
    "#     llm_int8_enable_fp32_cpu_offload=True\n",
    "# )\n",
    "\n",
    "processor = FuyuProcessor.from_pretrained(model_id, cache_dir=CACHE_DIR)\n",
    "model = FuyuForCausalLM.from_pretrained(model_id, cache_dir=CACHE_DIR, device_map=\"cuda:0\", torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1a9157b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:71013 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A blue bus parked on the side of a road.']\n"
     ]
    }
   ],
   "source": [
    "# prepare inputs for the model\n",
    "text_prompt = \"Generate a coco-style caption.\\n\"\n",
    "url = \"https://huggingface.co/adept/fuyu-8b/resolve/main/bus.png\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "inputs = processor(text=text_prompt, images=image, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "generation_output = model.generate(**inputs, max_new_tokens=7)\n",
    "generation_text = processor.batch_decode(generation_output[:, -7:], skip_special_tokens=True)\n",
    "\n",
    "print(generation_text)\n",
    "assert generation_text == ['A blue bus parked on the side of a road.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e6843dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:71013 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The bus is blue.\\n']\n"
     ]
    }
   ],
   "source": [
    "text_prompt = \"What color is the bus?\\n\"\n",
    "url = \"https://huggingface.co/adept/fuyu-8b/resolve/main/bus.png\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "inputs = processor(text=text_prompt, images=image, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "generation_output = model.generate(**inputs, max_new_tokens=6)\n",
    "generation_text = processor.batch_decode(generation_output[:, -6:], skip_special_tokens=True)\n",
    "\n",
    "print(generation_text)\n",
    "assert generation_text == [\"The bus is blue.\\n\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19efc8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:71013 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The life expectancy at birth for males is 80.2, while the highest life expectancy at']\n"
     ]
    }
   ],
   "source": [
    "text_prompt = \"What is the highest life expectancy at birth of male?\\n\"\n",
    "url = \"https://huggingface.co/adept/fuyu-8b/resolve/main/chart.png\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "model_inputs = processor(text=text_prompt, images=image, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "generation_output = model.generate(**model_inputs, max_new_tokens=16)\n",
    "generation_text = processor.batch_decode(generation_output[:, -16:], skip_special_tokens=True)\n",
    "\n",
    "print(generation_text)\n",
    "assert generation_text == [\"The life expectancy at birth of males in 2018 is 80.7.\\n\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
